import { useState } from 'react';
import { ChevronDown, ChevronRight } from 'lucide-react';

const steps = [
  {
    number: 1,
    title: 'Define Construct & Collect Content',
    description: 'Clearly define the construct you want to measure and collect the content that needs to be coded.',
    details: `Understanding Your Measurement Goal

Before implementing an SCC, you need to clearly define what observer judgment you want to measure. SCCs are specifically designed for observer judgments - properties that human coders can reliably assess when viewing content.

Appropriate constructs for SCCs include:
â€¢ Visual properties (e.g., image quality, emotional expressiveness in faces)
â€¢ Content attributes (e.g., food palatability, advertisement style)
â€¢ Perceptual qualities (e.g., trustworthiness, professionalism)

SCCs are NOT appropriate for:
â€¢ Internal states of content producers
â€¢ Behavioral outcomes of content consumers
â€¢ Constructs requiring context beyond the content itself

The Criterion Test

A construct qualifies as an observer judgment if a blinded human coder, given only the content and a coding protocol, can produce reliable ratings. If humans can't consistently code it, neither can an SCC.

Content Collection Considerations

Data Requirements:
â€¢ Your content should represent the full scope of what you'll eventually code
â€¢ Include sufficient variety to capture the range of your construct
â€¢ Consider edge cases and boundary conditions

Practical Guidelines:
â€¢ Start with at least 200-300 items for robust validation
â€¢ Ensure content quality (resolution, completeness, accessibility)
â€¢ Document any exclusion criteria or preprocessing steps

Setting Boundaries

SCCs are validated for specific contexts. Define your boundaries clearly:
â€¢ Content domain (e.g., restaurant reviews vs. product reviews)
â€¢ Platform/source (e.g., Yelp images vs. Instagram posts)
â€¢ Time period (if temporal factors matter)
â€¢ Population scope (whose perceptions you're modeling)

Remember: An SCC validated on one domain cannot be applied to another without revalidation. These boundaries aren't limitationsâ€”they ensure measurement validity.`
  },
  {
    number: 2,
    title: 'Separate Development & Validation Data',
    description: 'Split your dataset into separate development and validation sets to ensure proper evaluation.',
    details: `Partition Your Data

Why Partitioning Matters

Proper data partitioning is crucial for obtaining unbiased estimates of SCC performance. The partitioning strategy differs based on whether you're using an unfinetuned or finetuned SCC.

Unfinetuned SCCs: Two-Set Structure

For unfinetuned SCCs (using prompts without updating model weights), you need:

1. Test Set (Development)
â€¢ Used to develop and refine prompts
â€¢ Test different temperature settings
â€¢ Evaluate multiple prompt variations
â€¢ Size: ~100-150 items typically sufficient

2. Validation Set
â€¢ Held out for final, unbiased performance assessment
â€¢ Never accessed during development
â€¢ Used only once after finalizing prompts
â€¢ Size: ~100-150 items for stable estimates

3. Coding Set
â€¢ The remaining data you'll code after validation
â€¢ No human ratings needed for this set

Finetuned SCCs: Three-Set Structure

For finetuned SCCs (updating model parameters), you need:

1. Training Set
â€¢ Used to update model parameters
â€¢ Requires human ratings
â€¢ Size: Depends on complexity (200+ items common)

2. Test Set
â€¢ Evaluate performance during development
â€¢ Tune hyperparameters
â€¢ Size: ~100-150 items

3. Validation Set
â€¢ Final unbiased assessment
â€¢ Size: ~100-150 items

4. Coding Set
â€¢ Remaining data for final coding

Sampling Strategy

Random Sampling:
â€¢ Default approach for most applications
â€¢ Ensures each set represents the full distribution
â€¢ Use a fixed random seed for reproducibility

Stratified Sampling:
â€¢ When specific subgroups matter (e.g., content categories)
â€¢ Ensures representation across important dimensions
â€¢ Document stratification variables clearly

Size Considerations

The paper's analysis (Web Appendix 1) shows:
â€¢ 100 images achieve correlations within 0.001 of full sample
â€¢ Standard deviation of 0.037
â€¢ 98% of correlations between r=0.70-0.87

Practical recommendations:
â€¢ Minimum 100 items per set for stable estimates
â€¢ Larger sets (150-200) if resources permit
â€¢ Consider your tolerance for uncertainty

Critical Rules

âš ï¸ Never violate these principles:
â€¢ No peeking: Never look at validation set performance during development
â€¢ One shot: Validation set can only be used once
â€¢ Document everything: Record exact partitioning procedure
â€¢ Maintain separation: Keep sets completely independent

Bootstrap Enhancement

While developing on the test set, use bootstrapping to estimate validation performance:
â€¢ Resample test set (with replacement) 1,000+ times
â€¢ Calculate performance metrics for each sample
â€¢ Use median and confidence intervals to guide decisions
â€¢ Helps avoid overfitting to test set idiosyncrasies`
  },
  {
    number: 3,
    title: 'Collect Human Criterion Data (Development)',
    description: 'Gather human ratings on the development data to establish the criterion standard.',
    details: `The Criterion Standard

Human ratings serve as your "ground truth" for validation. The quality of your SCC can never exceed the quality of your criterion dataâ€”garbage in, garbage out.

Who Should Code?

Options for human coders:

1. Trained Expert Coders
â€¢ Best for complex or specialized constructs
â€¢ Higher inter-rater reliability
â€¢ More expensive and time-consuming
â€¢ Example from paper: Emotional expressiveness study used two trained coders (IRR = 0.9)

2. Crowdsourced Panels (MTurk, Prolific)
â€¢ Cost-effective for large samples
â€¢ Access to diverse coder populations
â€¢ Suitable for intuitive constructs
â€¢ Example from paper: Image quality and food palatability used MTurk workers

3. Student Coders
â€¢ Good for academic research settings
â€¢ Can be trained for consistency
â€¢ Balance of cost and quality

How Many Coders Per Item?

Minimum recommendations:
â€¢ Simple constructs: 3-5 coders per item
â€¢ Complex constructs: 5-10 coders per item
â€¢ Subjective constructs: 10+ coders per item

From the paper's examples:
â€¢ Food palatability: 35 raters per image (averaged)
â€¢ Image quality: Multiple raters per image via MTurk
â€¢ Emotional expressiveness: 2 trained coders (high agreement)

Creating Your Coding Protocol

Essential elements:
â€¢ Clear instructions matching what you'll give the SCC
â€¢ Scale definitions with anchors clearly specified
â€¢ Example items (if not contaminating test/validation sets)
â€¢ Quality checks to identify careless responses

Quality Control Measures

Pre-coding:
â€¢ Training items to familiarize coders with scale range
â€¢ Comprehension checks
â€¢ Clear inclusion/exclusion criteria

During coding:
â€¢ Attention checks (trap questions)
â€¢ Time stamps (flag too-fast responses)
â€¢ Consistency checks (repeated items)

Post-coding:
â€¢ Remove outlier coders
â€¢ Check inter-rater reliability
â€¢ Document all exclusions

Platform Considerations

MTurk/Prolific/CloudResearch:
â€¢ Qualifications (approval rate, location, language)
â€¢ Fair payment (minimum $15/hour equivalent)
â€¢ Batch management for consistency

In-house coding:
â€¢ Training sessions
â€¢ Regular calibration meetings
â€¢ Monitoring for drift over time

Calculating Agreement

For continuous scales:
â€¢ Intraclass Correlation (ICC)
â€¢ Average pairwise correlations
â€¢ Standard deviation across raters

For categorical coding:
â€¢ Cohen's Kappa
â€¢ Fleiss' Kappa (multiple raters)
â€¢ Percent agreement (with caution)

Critical Considerations

Demographic Representation:

The paper found SCCs better matched population averages than subgroups:
â€¢ General correlation: r=0.714
â€¢ Male coders only: r=0.609
â€¢ Female coders only: r=0.644

ðŸ’¡ Implication: Ensure your coder pool matches your intended inference population.

Cost-Benefit Analysis

When to invest in more/better coders:
â€¢ High-stakes research claims
â€¢ Novel or ambiguous constructs
â€¢ Publication in top journals
â€¢ Small validation sets

When basic crowdsourcing suffices:
â€¢ Well-established constructs
â€¢ Large validation sets
â€¢ Preliminary research
â€¢ Clear, objective properties

Data Processing

Before comparison with SCC:
â€¢ Average across multiple coders per item
â€¢ Check distribution (floor/ceiling effects)
â€¢ Transform if necessary (but document)
â€¢ Create final criterion scores

Documentation Requirements

Record and report:
â€¢ Coder demographics
â€¢ Payment and incentives
â€¢ Exact instructions provided
â€¢ Exclusion criteria and counts
â€¢ Inter-rater reliability metrics
â€¢ Processing decisions`
  },
  {
    number: 4,
    title: 'Create API Program',
    description: 'Build a program that queries the LLM via API with your specified prompts and parameters.',
    details: 'A computer program is generated which repeatedly calls or "talks to" the LLM through an Application Programming Interface (API) to collect responses. At this stage, researchers must generate a prompt to ask the SCC for ratings, decide on the specific LLM to use (e.g., GPT-4, Claude), and decide how to collect data (set temperature to 0 for consistent responses, or higher to average multiple responses).'
  },
  {
    number: 5,
    title: 'Implement Response Handling',
    description: 'Set up procedures to handle unexpected or malformed responses from the LLM.',
    details: 'While LLMs respond predictably most of the time, their probabilistic nature means SCCs occasionally respond unexpectedly. Researchers should include a cleaning stage in their program which either transforms or excludes unexpected responses. For example, if responses should be 1-7, the response "six" might be recoded as "6" while "B" would be discarded. Some LLMs can run in "structured output" mode to only allow specified responses.'
  },
  {
    number: 6,
    title: 'Collect SCC Ratings & Preregister',
    description: 'Collect Synthetic Content Coder ratings on validation data and preregister key information about your study.',
    details: 'Using the system developed, SCC responses are collected on the validation dataset. Preregistering your validation approach, metrics, and thresholds before examining results helps ensure transparency and prevents p-hacking or selective reporting.'
  },
  {
    number: 7,
    title: 'Collect Human Criterion Data (Validation)',
    description: 'Gather human ratings on the validation set for comparison with SCC ratings.',
    details: 'Human coders rate the validation set using the same instructions and scales as the development set. These independent human ratings provide the ground truth for validating your SCC performance on unseen data.'
  },
  {
    number: 8,
    title: 'Compare SCC & Human Ratings',
    description: 'Evaluate the SCC performance by comparing ratings with human criterion data using appropriate metrics.',
    details: 'SCC-coded and human-coded data are compared using relevant validation metrics such as correlation coefficients (Pearson or Spearman) and Root Mean Square Error (RMSE). Correlations show how well the SCC captures relative differences between items, while RMSE indicates absolute accuracy. Bootstrap resampling can provide confidence intervals for these metrics.'
  },
  {
    number: 9,
    title: 'Deploy or Start Over',
    description: 'Apply the validated SCC to your full dataset, or start over if validation is insufficient.',
    details: 'If validation metrics are sufficiently high, the protocol can be applied to code the entire content database. If validation metrics fall short, researchers may adjust the prompt, modify cleaning procedures, or undertake more complex procedures like fine-tuning. In many cases, it will be necessary to adjust aspects of the system setup, cleaning, and validation stages before final implementation.'
  }
];

export default function OverviewSection() {
  const [expandedStep, setExpandedStep] = useState(null);
  const [expandedSubsections, setExpandedSubsections] = useState({});

  const toggleStep = (number) => {
    setExpandedStep(expandedStep === number ? null : number);
  };

  const toggleSubsection = (stepNum, sectionIndex) => {
    const key = `${stepNum}-${sectionIndex}`;
    setExpandedSubsections(prev => ({
      ...prev,
      [key]: !prev[key]
    }));
  };

  // Parse details text into structured sections
  const parseDetails = (text) => {
    const lines = text.split('\n');
    const sections = [];
    let currentSection = null;
    let currentParagraph = [];
    let currentList = [];

    const flushParagraph = () => {
      if (currentParagraph.length > 0) {
        if (!currentSection) {
          currentSection = { title: null, content: [] };
        }
        currentSection.content.push({
          type: 'paragraph',
          text: currentParagraph.join(' ')
        });
        currentParagraph = [];
      }
    };

    const flushList = () => {
      if (currentList.length > 0) {
        if (!currentSection) {
          currentSection = { title: null, content: [] };
        }
        currentSection.content.push({
          type: 'list',
          items: [...currentList]
        });
        currentList = [];
      }
    };

    const flushSection = () => {
      flushParagraph();
      flushList();
      if (currentSection && currentSection.content.length > 0) {
        sections.push(currentSection);
        currentSection = null;
      }
    };

    lines.forEach((line) => {
      const trimmed = line.trim();

      // Empty line
      if (!trimmed) {
        flushParagraph();
        flushList();
        return;
      }

      // Bullet point
      if (trimmed.startsWith('â€¢')) {
        flushParagraph();
        currentList.push(trimmed.substring(1).trim());
        return;
      }

      // Numbered list item
      if (/^\d+\.\s/.test(trimmed)) {
        flushParagraph();
        currentList.push(trimmed);
        return;
      }

      // Section header
      const looksLikeHeader = trimmed.length < 50 && !trimmed.endsWith('.') && !trimmed.endsWith(',') && !trimmed.startsWith('âš ï¸');

      if (looksLikeHeader) {
        flushSection();
        currentSection = { title: trimmed, content: [] };
        return;
      }

      // Regular text
      flushList();
      currentParagraph.push(trimmed);
    });

    // Flush remaining content
    flushSection();

    return sections;
  };

  const renderSectionContent = (content, key) => {
    return content.map((item, i) => {
      if (item.type === 'paragraph') {
        return (
          <p key={`${key}-p-${i}`} className="text-slate-700 leading-relaxed mb-4">
            {item.text}
          </p>
        );
      }
      if (item.type === 'list') {
        return (
          <ul key={`${key}-ul-${i}`} className="list-disc list-inside text-slate-700 leading-relaxed mb-4 space-y-1">
            {item.items.map((listItem, j) => (
              <li key={j}>{listItem}</li>
            ))}
          </ul>
        );
      }
      return null;
    });
  };
  return (
    <div className="space-y-8">
      {/* Hero Section */}
      <div className="bg-gradient-to-r from-primary to-accent rounded-lg shadow-lg p-8">
        <h2 className="text-3xl font-bold mb-4 text-slate-900">
          Developing Synthetic Content Coders
        </h2>
        <p className="text-lg leading-relaxed text-slate-900">
          For unfinetuned SCCs, development involves a systematic 9-step process
          to ensure reliability and validity. Follow these steps to create, validate,
          and deploy LLM-based content coding systems for your research.
        </p>
      </div>

      {/* Steps Grid */}
      <div className="space-y-6">
        <h3 className="text-2xl font-bold text-slate-900">
          Implementation Steps
        </h3>

        <div className="grid grid-cols-1 gap-4">
          {steps.map((step) => {
            const isExpanded = expandedStep === step.number;

            return (
              <div
                key={step.number}
                className="bg-white rounded-lg shadow-md border border-slate-200 hover:shadow-lg transition-all duration-200 hover:border-accent/50"
              >
                {/* Clickable Header */}
                <button
                  onClick={() => toggleStep(step.number)}
                  className="w-full p-6 text-left focus:outline-none focus:ring-2 focus:ring-accent/50 rounded-lg"
                >
                  <div className="flex items-start gap-4">
                    {/* Step Number Badge */}
                    <div className="flex-shrink-0 w-12 h-12 rounded-full bg-primary text-white flex items-center justify-center font-bold text-lg">
                      {step.number}
                    </div>

                    {/* Content */}
                    <div className="flex-1">
                      <h4 className="font-bold text-lg text-slate-900 mb-2">
                        {step.title}
                      </h4>
                      <p className="text-slate-600 leading-relaxed">
                        {step.description}
                      </p>
                    </div>

                    {/* Chevron Icon */}
                    <div className="flex-shrink-0 text-slate-400">
                      {isExpanded ? (
                        <ChevronDown size={24} />
                      ) : (
                        <ChevronRight size={24} />
                      )}
                    </div>
                  </div>
                </button>

                {/* Expandable Details */}
                {isExpanded && (
                  <div className="px-6 pb-6 pt-4 border-t border-slate-100 mt-2">
                    <div className="pl-16 space-y-3">
                      {parseDetails(step.details).map((section, sectionIndex) => {
                        const subsectionKey = `${step.number}-${sectionIndex}`;
                        const isSubsectionExpanded = expandedSubsections[subsectionKey];

                        // If no title, just render content directly (no foldout)
                        if (!section.title) {
                          return (
                            <div key={sectionIndex}>
                              {renderSectionContent(section.content, subsectionKey)}
                            </div>
                          );
                        }

                        // Render as collapsible subsection
                        return (
                          <div key={sectionIndex} className="border-l-2 border-slate-200 pl-4">
                            <button
                              onClick={() => toggleSubsection(step.number, sectionIndex)}
                              className="w-full text-left flex items-center gap-2 py-2 hover:text-accent transition-colors"
                            >
                              <div className="flex-shrink-0 text-slate-400">
                                {isSubsectionExpanded ? (
                                  <ChevronDown size={18} />
                                ) : (
                                  <ChevronRight size={18} />
                                )}
                              </div>
                              <h4 className="font-bold text-slate-900 text-base">
                                {section.title}
                              </h4>
                            </button>

                            {isSubsectionExpanded && (
                              <div className="pl-6 mt-2">
                                {renderSectionContent(section.content, subsectionKey)}
                              </div>
                            )}
                          </div>
                        );
                      })}
                    </div>
                  </div>
                )}
              </div>
            );
          })}
        </div>
      </div>

      {/* Bottom CTA Section */}
      <div className="bg-slate-100 border border-slate-200 rounded-lg p-6 mt-8">
        <h3 className="font-bold text-slate-900 mb-3 text-lg">Ready to Get Started?</h3>
        <div className="space-y-2 text-slate-700">
          <p>
            <strong>Tutorial:</strong> See step-by-step code examples for different content types
          </p>
          <p>
            <strong>Code Generator:</strong> Generate customized Python code for GPT-4 or Claude
          </p>
          <p>
            <strong>Resources:</strong> Access validation tools, templates, and best practices
          </p>
        </div>
      </div>
    </div>
  );
}
